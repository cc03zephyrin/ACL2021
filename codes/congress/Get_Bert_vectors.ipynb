{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Bert vectors.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0a4mTk9o1Qg"
      },
      "source": [
        "# Copyright 2019 Google Inc.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ"
      },
      "source": [
        "#Predicting Movie Review Sentiment with BERT on TF Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYCHQX5GjUcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ae6455e-052c-4107-cf9a-bce8de1e16a6"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15\n",
        "!pip install bert-tensorflow==1.0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 1.15.0\n",
            "Uninstalling tensorflow-1.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-1.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "Collecting tensorflow==1.15\n",
            "  Using cached tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.39.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorflow-1.15.0\n",
            "Requirement already satisfied: bert-tensorflow==1.0.1 in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow==1.0.1) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umWbjSJhjsVv"
      },
      "source": [
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac6f8bc-b4a4-4983-a080-fe2756bb1a47"
      },
      "source": [
        "import bert\n",
        "\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thXa-7wMzjq9"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO2pNR-LGOrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34037916-94d0-43bf-fe5b-d905ba3c919a"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hls_lXPDGb3X",
        "outputId": "ffb4f8b1-7b3f-4de5-afee-6fcfe454bb50"
      },
      "source": [
        "\n",
        "import os\n",
        "print(os.listdir())\n",
        "os.chdir(r'./gdrive/MyDrive/congress_SSL')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'gdrive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZJoKvFrMjcd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax4RvszCHBGC",
        "outputId": "0cbcd995-d44b-4917-d353-ef1ca680d030"
      },
      "source": [
        "print(os.listdir())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['somas', 'Bert_somas.ipynb', 'congress_data', 'results', 'labelspreading.ipynb', 'saved_bert_model', 'Bert prediction.ipynb', 'saved_bert_model2', 'Bert vectors.ipynb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vPjjXnmVvTO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCWAymn0k_Oj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVB3eOcjxxm1"
      },
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
        "\n",
        "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc8f346-0a3f-4759-b18c-361ef4737b1d"
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = r'./saved_bert_model2/'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = False #@param {type:\"boolean\"}\n",
        "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "USE_BUCKET = False #@param {type:\"boolean\"}\n",
        "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: ./saved_bert_model2/ *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISTzOV2AVwJG",
        "outputId": "06bfeb3b-ce5b-4111-ac78-8480eb6d154b"
      },
      "source": [
        "print(os.listdir())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['somas', 'Bert_somas.ipynb', 'congress_data', 'results', 'labelspreading.ipynb', 'saved_bert_model', 'Bert prediction.ipynb', 'saved_bert_model2', 'Bert vectors.ipynb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_w8SRqN0fr"
      },
      "source": [
        "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom_ff20gyy6"
      },
      "source": [
        "\n",
        "\n",
        "# # Load all files from a directory in a DataFrame.\n",
        "# def load_directory_data(directory):\n",
        "#   data = {}\n",
        "#   data[\"sentence\"] = []\n",
        "#   data[\"sentiment\"] = []\n",
        "#   for file_path in os.listdir(directory):\n",
        "#     with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "#       data[\"sentence\"].append(f.read())\n",
        "#       data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "#   return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# # Merge positive and negative examples, add a polarity column and shuffle.\n",
        "# def load_dataset(directory):\n",
        "#   pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "#   neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "#   pos_df[\"polarity\"] = 1\n",
        "#   neg_df[\"polarity\"] = 0\n",
        "#   return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# # Download and process the dataset files.\n",
        "# def download_and_load_datasets(force_download=False):\n",
        "#   dataset = tf.keras.utils.get_file(\n",
        "#       fname=\"aclImdb.tar.gz\", \n",
        "#       origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "#       extract=True)\n",
        "  \n",
        "#   train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "#                                        \"aclImdb\", \"train\"))\n",
        "#   test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "#                                       \"aclImdb\", \"test\"))\n",
        "  \n",
        "#   return train_df, test_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQgIm26Pknx"
      },
      "source": [
        "congress = pd.read_csv(r'./congress_data/crs_2009_20_10_04_trancated.csv',  sep ='\\t', encoding = 'ISO-8859-1', index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXvpFLGYQOPJ"
      },
      "source": [
        "dws = np.load(r'./congress_data/dws_2009_20.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKeEnIsYQiem"
      },
      "source": [
        "congress['dw']= dws"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "NGk2rD7KsFeH",
        "outputId": "fc89592e-fa7f-4d44-f17f-d93b7a63032c"
      },
      "source": [
        "congress.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speech_id</th>\n",
              "      <th>true_id</th>\n",
              "      <th>chamber</th>\n",
              "      <th>state</th>\n",
              "      <th>party</th>\n",
              "      <th>congress</th>\n",
              "      <th>daysafter</th>\n",
              "      <th>monthsafter</th>\n",
              "      <th>speech</th>\n",
              "      <th>date</th>\n",
              "      <th>topics_prior</th>\n",
              "      <th>dw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>594060</th>\n",
              "      <td>1110000007</td>\n",
              "      <td>L000557</td>\n",
              "      <td>H</td>\n",
              "      <td>CT</td>\n",
              "      <td>D</td>\n",
              "      <td>111</td>\n",
              "      <td>6578</td>\n",
              "      <td>216</td>\n",
              "      <td>our democracy renews itself every 2 years as m...</td>\n",
              "      <td>20090106</td>\n",
              "      <td>22</td>\n",
              "      <td>-0.398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594061</th>\n",
              "      <td>1110000009</td>\n",
              "      <td>P000587</td>\n",
              "      <td>H</td>\n",
              "      <td>IN</td>\n",
              "      <td>R</td>\n",
              "      <td>111</td>\n",
              "      <td>6578</td>\n",
              "      <td>216</td>\n",
              "      <td>madam clerk. as chairman of the republican con...</td>\n",
              "      <td>20090106</td>\n",
              "      <td>22</td>\n",
              "      <td>0.655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594062</th>\n",
              "      <td>1110000013</td>\n",
              "      <td>B000589</td>\n",
              "      <td>H</td>\n",
              "      <td>OH</td>\n",
              "      <td>R</td>\n",
              "      <td>111</td>\n",
              "      <td>6578</td>\n",
              "      <td>216</td>\n",
              "      <td>. leader hoyer. fellow members. and a special ...</td>\n",
              "      <td>20090106</td>\n",
              "      <td>18</td>\n",
              "      <td>0.513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594063</th>\n",
              "      <td>1110000014</td>\n",
              "      <td>P000197</td>\n",
              "      <td>H</td>\n",
              "      <td>CA</td>\n",
              "      <td>D</td>\n",
              "      <td>111</td>\n",
              "      <td>6578</td>\n",
              "      <td>216</td>\n",
              "      <td>thank you very much. leader boehner. together....</td>\n",
              "      <td>20090106</td>\n",
              "      <td>22</td>\n",
              "      <td>-0.490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594064</th>\n",
              "      <td>1110000016</td>\n",
              "      <td>L000557</td>\n",
              "      <td>H</td>\n",
              "      <td>CT</td>\n",
              "      <td>D</td>\n",
              "      <td>111</td>\n",
              "      <td>6578</td>\n",
              "      <td>216</td>\n",
              "      <td>for\"your nomination this morning. thank you to...</td>\n",
              "      <td>20090106</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.398</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         speech_id  true_id chamber  ...      date topics_prior     dw\n",
              "594060  1110000007  L000557       H  ...  20090106           22 -0.398\n",
              "594061  1110000009  P000587       H  ...  20090106           22  0.655\n",
              "594062  1110000013  B000589       H  ...  20090106           18  0.513\n",
              "594063  1110000014  P000197       H  ...  20090106           22 -0.490\n",
              "594064  1110000016  L000557       H  ...  20090106           -1 -0.398\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeXOEJvKQnLS"
      },
      "source": [
        "\n",
        "true_switchers_ = ['A000361','B000229', 'B001264', 'D000168', 'G000280', 'H000067', 'H000390', 'L000119', 'P000066', 'T000058', 'C000077','F000257', 'G000557', \n",
        "'S000320', 'S000709','J000072']\n",
        "\n",
        "independents = ['S000033', 'B001237', 'K000383']\n",
        "\n",
        "true_switchers =independents + true_switchers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZnvpC6VQuA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca70da5-7b49-40ad-ab9e-8adccf136921"
      },
      "source": [
        "# filter out all switchers\n",
        "congress_flt_ = congress.loc[~congress['true_id'].isin(true_switchers)]\n",
        "congress_flt_['is_Dem'] = (congress_flt_['party'] == 'D')*1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYK3ru2_9-vH"
      },
      "source": [
        "# for test, comment this for real training\n",
        "\n",
        "\n",
        "# congress_flt = congress_flt_.sample(5000)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_HOI5TeDFjN"
      },
      "source": [
        "congress_flt = congress_flt_.sort_values(by = ['dw'], ascending = True, ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwiJYjl5Ltgc"
      },
      "source": [
        "congress_flt.to_csv(r'./congress_data/congress_sorted_2009_20_10_04.csv', sep = '\\t', index_label= False, encoding = 'ISO-8859-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8j54DwzwZE1",
        "outputId": "9b57cd7d-1912-450d-d20c-4e3cf7432928"
      },
      "source": [
        "congress_flt[:5], congress_flt[-5:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(    speech_id  true_id chamber state  ...      date  topics_prior     dw  is_Dem\n",
              " 0  1130067801  W000817       S    MA  ...  20140107            23 -0.769       1\n",
              " 1  1130082890  W000817       S    MA  ...  20140327            45 -0.769       1\n",
              " 2  1140095053  W000817       S    MA  ...  20160523            64 -0.769       1\n",
              " 3  1130074735  W000817       S    MA  ...  20140212            18 -0.769       1\n",
              " 4  1130074743  W000817       S    MA  ...  20140212            64 -0.769       1\n",
              " \n",
              " [5 rows x 13 columns],\n",
              "          speech_id  true_id chamber  ... topics_prior     dw  is_Dem\n",
              " 206894  1130048513  L000577       S  ...           28  0.916       0\n",
              " 206895  1140093911  L000577       S  ...           30  0.916       0\n",
              " 206896  1140004428  L000577       S  ...           -1  0.916       0\n",
              " 206897  1140094358  L000577       S  ...           24  0.916       0\n",
              " 206898  1140070318  L000577       S  ...           22  0.916       0\n",
              " \n",
              " [5 rows x 13 columns])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hbwNbAvzEhp"
      },
      "source": [
        "def sample(df, biased = True, notmask = 0.8):\n",
        "  parties = df['is_Dem'].values\n",
        "  topics = df['topics_prior'].values\n",
        "  total = len(df)\n",
        "  print(total)\n",
        "  if not biased: \n",
        "    print('started random split, test size {}'.format(1-notmask))\n",
        "    sss = StratifiedShuffleSplit(n_splits=5, test_size= 1- notmask)\n",
        "    trainid, testid = next(sss.split(topics, parties))\n",
        "    assert len(trainid) + len(testid) == total, \"length error\"\n",
        "  else:\n",
        "    print('started biased split, test size {}'.format(1-notmask))\n",
        "    topn = int(total * notmask/2.0)\n",
        "    testid = np.arange(total)[topn:-topn]\n",
        "    trainid = np.concatenate([np.arange(total)[:topn], np.arange(total)[-topn:]])\n",
        "  return trainid, testid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3crSpcM8KIM",
        "outputId": "01dff0e3-79b3-40ec-ae2f-ce4a2fad3420"
      },
      "source": [
        "trainid, testid = sample(congress_flt, biased = False, notmask = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "206899\n",
            "started random split, test size 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abfwdn-g135"
      },
      "source": [
        "\n",
        "\n",
        "# train, test = download_and_load_datasets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRQM8pDi8xI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675a9629-9cca-4bcf-ab0f-f6f4da30284e"
      },
      "source": [
        "congress_flt.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['speech_id', 'true_id', 'chamber', 'state', 'party', 'congress',\n",
              "       'daysafter', 'monthsafter', 'speech', 'date', 'topics_prior', 'dw',\n",
              "       'is_Dem'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRnHSz3iSXz"
      },
      "source": [
        "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it"
      },
      "source": [
        "DATA_COLUMN = 'speech'\n",
        "LABEL_COLUMN = 'is_Dem'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z"
      },
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6"
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "# train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "# test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "#                                                                    text_a = x[DATA_COLUMN], \n",
        "#                                                                    text_b = None, \n",
        "#                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
        "traintest_InputExamples = congress_flt.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W36ORcZnl2oW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a81148-65e7-4943-b05f-5924e539909c"
      },
      "source": [
        "traintest_InputExamples[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    <bert.run_classifier.InputExample object at 0x...\n",
              "1    <bert.run_classifier.InputExample object at 0x...\n",
              "2    <bert.run_classifier.InputExample object at 0x...\n",
              "3    <bert.run_classifier.InputExample object at 0x...\n",
              "4    <bert.run_classifier.InputExample object at 0x...\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZWZtKxObjh"
      },
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb39dd5-3e6e-47b9-c539-c61395212c09"
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oFkhpZBDKm"
      },
      "source": [
        "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBo6RCtQmwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f79090-4381-4eae-a827-9adff19cbc7e"
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer. And I love this.\\n But I think is tsrerereazrsaf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer',\n",
              " '.',\n",
              " 'and',\n",
              " 'i',\n",
              " 'love',\n",
              " 'this',\n",
              " '.',\n",
              " 'but',\n",
              " 'i',\n",
              " 'think',\n",
              " 'is',\n",
              " 'ts',\n",
              " '##rer',\n",
              " '##ere',\n",
              " '##az',\n",
              " '##rsa',\n",
              " '##f']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc"
      },
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a2305c8-1829-4516-8dbe-4dce3ce11cdb"
      },
      "source": [
        "# We'll set sequences to be at most 256 tokens long.\n",
        "MAX_SEQ_LENGTH = 150\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "traintest_features = np.array(bert.run_classifier.convert_examples_to_features(traintest_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer))\n",
        "# train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "# test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 206899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 206899\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i am here today with some good news . this week the government will fix something that was broken . i know that some people wish to deny that is possible . but hear me out . five years ago . during the 2008 financial crisis . we witnessed first ##hand that the market for home mortgage ##s was badly broken . the fundamental problem was that many lend ##ers issued mortgage ##s without any concern about whether the borrow ##er would be able to repay those mortgage ##s in the long run . why would they do that ? they did it because they could immediately sell the mortgage to another financial institution . if the borrow ##er couldn ##t pay . that would turn out to be somebody else ##s problem . we all know what happened next . millions of these dangerous mortgage ##s were [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i am here today with some good news . this week the government will fix something that was broken . i know that some people wish to deny that is possible . but hear me out . five years ago . during the 2008 financial crisis . we witnessed first ##hand that the market for home mortgage ##s was badly broken . the fundamental problem was that many lend ##ers issued mortgage ##s without any concern about whether the borrow ##er would be able to repay those mortgage ##s in the long run . why would they do that ? they did it because they could immediately sell the mortgage to another financial institution . if the borrow ##er couldn ##t pay . that would turn out to be somebody else ##s problem . we all know what happened next . millions of these dangerous mortgage ##s were [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 2572 2182 2651 2007 2070 2204 2739 1012 2023 2733 1996 2231 2097 8081 2242 2008 2001 3714 1012 1045 2113 2008 2070 2111 4299 2000 9772 2008 2003 2825 1012 2021 2963 2033 2041 1012 2274 2086 3283 1012 2076 1996 2263 3361 5325 1012 2057 9741 2034 11774 2008 1996 3006 2005 2188 14344 2015 2001 6649 3714 1012 1996 8050 3291 2001 2008 2116 18496 2545 3843 14344 2015 2302 2151 5142 2055 3251 1996 17781 2121 2052 2022 2583 2000 24565 2216 14344 2015 1999 1996 2146 2448 1012 2339 2052 2027 2079 2008 1029 2027 2106 2009 2138 2027 2071 3202 5271 1996 14344 2000 2178 3361 5145 1012 2065 1996 17781 2121 2481 2102 3477 1012 2008 2052 2735 2041 2000 2022 8307 2842 2015 3291 1012 2057 2035 2113 2054 3047 2279 1012 8817 1997 2122 4795 14344 2015 2020 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 2572 2182 2651 2007 2070 2204 2739 1012 2023 2733 1996 2231 2097 8081 2242 2008 2001 3714 1012 1045 2113 2008 2070 2111 4299 2000 9772 2008 2003 2825 1012 2021 2963 2033 2041 1012 2274 2086 3283 1012 2076 1996 2263 3361 5325 1012 2057 9741 2034 11774 2008 1996 3006 2005 2188 14344 2015 2001 6649 3714 1012 1996 8050 3291 2001 2008 2116 18496 2545 3843 14344 2015 2302 2151 5142 2055 3251 1996 17781 2121 2052 2022 2583 2000 24565 2216 14344 2015 1999 1996 2146 2448 1012 2339 2052 2027 2079 2008 1029 2027 2106 2009 2138 2027 2071 3202 5271 1996 14344 2000 2178 3361 5145 1012 2065 1996 17781 2121 2481 2102 3477 1012 2008 2052 2735 2041 2000 2022 8307 2842 2015 3291 1012 2057 2035 2113 2054 3047 2279 1012 8817 1997 2122 4795 14344 2015 2020 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . this is a difficult day for the city of boston . yesterday boston lost two courageous firefighters who gave up their lives battling a terrible fire in the city ##s back bay . when others flee . our firefighters rush head ##long into danger . concerned only for the safety of others . they put their lives on the line every time . today we mo ##urn the loss of two brave men . two heroes who made the ultimate sacrifice . lieutenant ed walsh and fire ##fighter mike kennedy were highly respected and committed members of the boston fire department who dedicated their lives to keeping our families safe . fire ##fighter kennedy of ladder company 15 on boy ##lston street was a member of the boston fire department for 61 / 2 years . he grew up in ro ##slin ##dale . served our country [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . this is a difficult day for the city of boston . yesterday boston lost two courageous firefighters who gave up their lives battling a terrible fire in the city ##s back bay . when others flee . our firefighters rush head ##long into danger . concerned only for the safety of others . they put their lives on the line every time . today we mo ##urn the loss of two brave men . two heroes who made the ultimate sacrifice . lieutenant ed walsh and fire ##fighter mike kennedy were highly respected and committed members of the boston fire department who dedicated their lives to keeping our families safe . fire ##fighter kennedy of ladder company 15 on boy ##lston street was a member of the boston fire department for 61 / 2 years . he grew up in ro ##slin ##dale . served our country [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 2023 2003 1037 3697 2154 2005 1996 2103 1997 3731 1012 7483 3731 2439 2048 26103 21767 2040 2435 2039 2037 3268 17773 1037 6659 2543 1999 1996 2103 2015 2067 3016 1012 2043 2500 10574 1012 2256 21767 5481 2132 10052 2046 5473 1012 4986 2069 2005 1996 3808 1997 2500 1012 2027 2404 2037 3268 2006 1996 2240 2296 2051 1012 2651 2057 9587 14287 1996 3279 1997 2048 9191 2273 1012 2048 7348 2040 2081 1996 7209 8688 1012 3812 3968 11019 1998 2543 20027 3505 5817 2020 3811 9768 1998 5462 2372 1997 1996 3731 2543 2533 2040 4056 2037 3268 2000 4363 2256 2945 3647 1012 2543 20027 5817 1997 10535 2194 2321 2006 2879 21540 2395 2001 1037 2266 1997 1996 3731 2543 2533 2005 6079 1013 1016 2086 1012 2002 3473 2039 1999 20996 22908 5634 1012 2366 2256 2406 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 2023 2003 1037 3697 2154 2005 1996 2103 1997 3731 1012 7483 3731 2439 2048 26103 21767 2040 2435 2039 2037 3268 17773 1037 6659 2543 1999 1996 2103 2015 2067 3016 1012 2043 2500 10574 1012 2256 21767 5481 2132 10052 2046 5473 1012 4986 2069 2005 1996 3808 1997 2500 1012 2027 2404 2037 3268 2006 1996 2240 2296 2051 1012 2651 2057 9587 14287 1996 3279 1997 2048 9191 2273 1012 2048 7348 2040 2081 1996 7209 8688 1012 3812 3968 11019 1998 2543 20027 3505 5817 2020 3811 9768 1998 5462 2372 1997 1996 3731 2543 2533 2040 4056 2037 3268 2000 4363 2256 2945 3647 1012 2543 20027 5817 1997 10535 2194 2321 2006 2879 21540 2395 2001 1037 2266 1997 1996 3731 2543 2533 2005 6079 1013 1016 2086 1012 2002 3473 2039 1999 20996 22908 5634 1012 2366 2256 2406 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . 8 years ago reckless bankers on wall street sparked a financial melt ##down . their too ##bi ##gt ##of ##ail banks gamble ##d with our economy . encouraging reckless mortgage lending by funding the slim ##y sub ##pr ##ime lend ##ers who pe ##ddled their miserable products to millions of american families . those same banks then go ##bbled up those dangerous mortgage ##s . rep ##ack ##aged them . and spread huge risks throughout the financial system . the consequences were disastrous . wall street greed destroyed $ 7 trillion in housing wealth and resulted in millions of americans losing their homes . it killed 8 . 7 million american jobs . it gut ##ted hundreds of pension funds . leaving millions of retire ##es hung out to dry . thanks to washington bail ##outs . wall street is once again flying high . corporate profits [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . 8 years ago reckless bankers on wall street sparked a financial melt ##down . their too ##bi ##gt ##of ##ail banks gamble ##d with our economy . encouraging reckless mortgage lending by funding the slim ##y sub ##pr ##ime lend ##ers who pe ##ddled their miserable products to millions of american families . those same banks then go ##bbled up those dangerous mortgage ##s . rep ##ack ##aged them . and spread huge risks throughout the financial system . the consequences were disastrous . wall street greed destroyed $ 7 trillion in housing wealth and resulted in millions of americans losing their homes . it killed 8 . 7 million american jobs . it gut ##ted hundreds of pension funds . leaving millions of retire ##es hung out to dry . thanks to washington bail ##outs . wall street is once again flying high . corporate profits [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1022 2086 3283 18555 25375 2006 2813 2395 13977 1037 3361 14899 7698 1012 2037 2205 5638 13512 11253 12502 5085 18503 2094 2007 2256 4610 1012 11434 18555 14344 18435 2011 4804 1996 11754 2100 4942 18098 14428 18496 2545 2040 21877 28090 2037 13736 3688 2000 8817 1997 2137 2945 1012 2216 2168 5085 2059 2175 12820 2039 2216 4795 14344 2015 1012 16360 8684 18655 2068 1012 1998 3659 4121 10831 2802 1996 3361 2291 1012 1996 8465 2020 16775 1012 2813 2395 22040 3908 1002 1021 23458 1999 3847 7177 1998 4504 1999 8817 1997 4841 3974 2037 5014 1012 2009 2730 1022 1012 1021 2454 2137 5841 1012 2009 9535 3064 5606 1997 11550 5029 1012 2975 8817 1997 11036 2229 5112 2041 2000 4318 1012 4283 2000 2899 15358 12166 1012 2813 2395 2003 2320 2153 3909 2152 1012 5971 11372 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1022 2086 3283 18555 25375 2006 2813 2395 13977 1037 3361 14899 7698 1012 2037 2205 5638 13512 11253 12502 5085 18503 2094 2007 2256 4610 1012 11434 18555 14344 18435 2011 4804 1996 11754 2100 4942 18098 14428 18496 2545 2040 21877 28090 2037 13736 3688 2000 8817 1997 2137 2945 1012 2216 2168 5085 2059 2175 12820 2039 2216 4795 14344 2015 1012 16360 8684 18655 2068 1012 1998 3659 4121 10831 2802 1996 3361 2291 1012 1996 8465 2020 16775 1012 2813 2395 22040 3908 1002 1021 23458 1999 3847 7177 1998 4504 1999 8817 1997 4841 3974 2037 5014 1012 2009 2730 1022 1012 1021 2454 2137 5841 1012 2009 9535 3064 5606 1997 11550 5029 1012 2975 8817 1997 11036 2229 5112 2041 2000 4318 1012 4283 2000 2899 15358 12166 1012 2813 2395 2003 2320 2153 3909 2152 1012 5971 11372 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . i thank senators du ##rbin and reed for their extraordinary leadership on this important issue . i also rise today to talk about the crushing burden which student debt places on our college students and on our economy . and i call on congress to address it . the core facts are well known to every family in america . in recent decades . college costs have sky ##rock ##ete ##d . adjusted for inflation . a young person today pays 300 percent of what their parents paid just 30 years ago . for millions of young people . the only way to cover this tuition cost is to take on huge debt . the average student loan balance among 25 ##year ##old ##s who borrow has grown by 91 percent in just 10 years . total outstanding student loan debt stands at a staggering $ 1 [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . i thank senators du ##rbin and reed for their extraordinary leadership on this important issue . i also rise today to talk about the crushing burden which student debt places on our college students and on our economy . and i call on congress to address it . the core facts are well known to every family in america . in recent decades . college costs have sky ##rock ##ete ##d . adjusted for inflation . a young person today pays 300 percent of what their parents paid just 30 years ago . for millions of young people . the only way to cover this tuition cost is to take on huge debt . the average student loan balance among 25 ##year ##old ##s who borrow has grown by 91 percent in just 10 years . total outstanding student loan debt stands at a staggering $ 1 [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1045 4067 10153 4241 27366 1998 7305 2005 2037 9313 4105 2006 2023 2590 3277 1012 1045 2036 4125 2651 2000 2831 2055 1996 14527 10859 2029 3076 7016 3182 2006 2256 2267 2493 1998 2006 2256 4610 1012 1998 1045 2655 2006 3519 2000 4769 2009 1012 1996 4563 8866 2024 2092 2124 2000 2296 2155 1999 2637 1012 1999 3522 5109 1012 2267 5366 2031 3712 16901 12870 2094 1012 10426 2005 14200 1012 1037 2402 2711 2651 12778 3998 3867 1997 2054 2037 3008 3825 2074 2382 2086 3283 1012 2005 8817 1997 2402 2111 1012 1996 2069 2126 2000 3104 2023 15413 3465 2003 2000 2202 2006 4121 7016 1012 1996 2779 3076 5414 5703 2426 2423 29100 11614 2015 2040 17781 2038 4961 2011 6205 3867 1999 2074 2184 2086 1012 2561 5151 3076 5414 7016 4832 2012 1037 26233 1002 1015 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1045 4067 10153 4241 27366 1998 7305 2005 2037 9313 4105 2006 2023 2590 3277 1012 1045 2036 4125 2651 2000 2831 2055 1996 14527 10859 2029 3076 7016 3182 2006 2256 2267 2493 1998 2006 2256 4610 1012 1998 1045 2655 2006 3519 2000 4769 2009 1012 1996 4563 8866 2024 2092 2124 2000 2296 2155 1999 2637 1012 1999 3522 5109 1012 2267 5366 2031 3712 16901 12870 2094 1012 10426 2005 14200 1012 1037 2402 2711 2651 12778 3998 3867 1997 2054 2037 3008 3825 2074 2382 2086 3283 1012 2005 8817 1997 2402 2111 1012 1996 2069 2126 2000 3104 2023 15413 3465 2003 2000 2202 2006 4121 7016 1012 1996 2779 3076 5414 5703 2426 2423 29100 11614 2015 2040 17781 2038 4961 2011 6205 3867 1999 2074 2184 2086 1012 2561 5151 3076 5414 7016 4832 2012 1037 26233 1002 1015 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . i would like to speak for another minute about the issue of ref ##ina ##nc ##ing student loans . this is real money back in the pockets of people who invested in their education . real money will help young people find a little more financial stability as they work hard to build their futures . real money that says america invest ##s in those who get an education . we don ##t need to add a single dime to our deficit to pay for this plan . right now this country essentially taxes students by charging high interest rates that bring money into the government while at the same time we give away far more money through a tax code riddle ##d with loop ##holes and let the wealthiest individuals and corporations avoid paying a fair share . we can close those loop ##holes and put [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] . i would like to speak for another minute about the issue of ref ##ina ##nc ##ing student loans . this is real money back in the pockets of people who invested in their education . real money will help young people find a little more financial stability as they work hard to build their futures . real money that says america invest ##s in those who get an education . we don ##t need to add a single dime to our deficit to pay for this plan . right now this country essentially taxes students by charging high interest rates that bring money into the government while at the same time we give away far more money through a tax code riddle ##d with loop ##holes and let the wealthiest individuals and corporations avoid paying a fair share . we can close those loop ##holes and put [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1045 2052 2066 2000 3713 2005 2178 3371 2055 1996 3277 1997 25416 3981 12273 2075 3076 10940 1012 2023 2003 2613 2769 2067 1999 1996 10306 1997 2111 2040 11241 1999 2037 2495 1012 2613 2769 2097 2393 2402 2111 2424 1037 2210 2062 3361 9211 2004 2027 2147 2524 2000 3857 2037 17795 1012 2613 2769 2008 2758 2637 15697 2015 1999 2216 2040 2131 2019 2495 1012 2057 2123 2102 2342 2000 5587 1037 2309 27211 2000 2256 15074 2000 3477 2005 2023 2933 1012 2157 2085 2023 2406 7687 7773 2493 2011 13003 2152 3037 6165 2008 3288 2769 2046 1996 2231 2096 2012 1996 2168 2051 2057 2507 2185 2521 2062 2769 2083 1037 4171 3642 21834 2094 2007 7077 19990 1998 2292 1996 27809 3633 1998 11578 4468 7079 1037 4189 3745 1012 2057 2064 2485 2216 7077 19990 1998 2404 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1012 1045 2052 2066 2000 3713 2005 2178 3371 2055 1996 3277 1997 25416 3981 12273 2075 3076 10940 1012 2023 2003 2613 2769 2067 1999 1996 10306 1997 2111 2040 11241 1999 2037 2495 1012 2613 2769 2097 2393 2402 2111 2424 1037 2210 2062 3361 9211 2004 2027 2147 2524 2000 3857 2037 17795 1012 2613 2769 2008 2758 2637 15697 2015 1999 2216 2040 2131 2019 2495 1012 2057 2123 2102 2342 2000 5587 1037 2309 27211 2000 2256 15074 2000 3477 2005 2023 2933 1012 2157 2085 2023 2406 7687 7773 2493 2011 13003 2152 3037 6165 2008 3288 2769 2046 1996 2231 2096 2012 1996 2168 2051 2057 2507 2185 2521 2062 2769 2083 1037 4171 3642 21834 2094 2007 7077 19990 1998 2292 1996 27809 3633 1998 11578 4468 7079 1037 4189 3745 1012 2057 2064 2485 2216 7077 19990 1998 2404 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f4765e17fd56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMAX_SEQ_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert our train and test features to InputFeatures that BERT understands.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraintest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraintest_InputExamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert/run_classifier.py\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, label_list, max_seq_length, tokenizer)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     feature = convert_single_example(ex_index, example, label_list,\n\u001b[0;32m--> 777\u001b[0;31m                                      max_seq_length, tokenizer)\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert/run_classifier.py\u001b[0m in \u001b[0;36mconvert_single_example\u001b[0;34m(ex_index, example, label_list, max_seq_length, tokenizer)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m   \u001b[0mtokens_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m   \u001b[0mtokens_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert/tokenization.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert/tokenization.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_strip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m       \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert/tokenization.py\u001b[0m in \u001b[0;36m_run_split_on_punc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mstart_new_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert/tokenization.py\u001b[0m in \u001b[0;36m_is_punctuation\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0;31m# Punctuation class but we treat them as punctuation anyways, for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m   \u001b[0;31m# consistency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m   if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n\u001b[0m\u001b[1;32m    394\u001b[0m       (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq"
      },
      "source": [
        "def create_model( input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=False)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer_ = bert_outputs[\"pooled_output\"]\n",
        "  output_layer = tf.identity(output_layer_, name = 'output')\n",
        "  out = tf.get_default_graph().get_tensor_by_name('output:0')\n",
        "  return out\n",
        "\n",
        "  # hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # # Create our own layer to tune for politeness data.\n",
        "  # output_weights = tf.get_variable(\n",
        "  #     \"output_weights\", [num_labels, hidden_size],\n",
        "  #     initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  # output_bias = tf.get_variable(\n",
        "  #     \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  # with tf.variable_scope(\"loss\"):\n",
        "\n",
        "  #   # Dropout helps prevent overfitting\n",
        "  #   output_layer_ = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "  #   logits = tf.matmul(output_layer_, output_weights, transpose_b=True)\n",
        "  #   logits = tf.nn.bias_add(logits, output_bias)\n",
        "  #   log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "  #   # Convert labels into one-hot encoding\n",
        "  #   one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "  #   predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "  #   # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "  #   if is_predicting:\n",
        "  #     return  output_layer\n",
        "  #   # If we're train/eval, compute loss between predicted and actual label\n",
        "  #   per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "  #   loss = tf.reduce_mean(per_example_loss)\n",
        "  #   return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW"
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    # is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # # TRAIN and EVAL\n",
        "    # if not is_predicting:\n",
        "\n",
        "    #   (loss, predicted_labels, log_probs) = create_model(\n",
        "    #     is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "    #   train_op = bert.optimization.create_optimizer(\n",
        "    #       loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "    #   # Calculate evaluation metrics. \n",
        "    #   def metric_fn(label_ids, predicted_labels):\n",
        "    #     accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "    #     f1_score = tf.contrib.metrics.f1_score(\n",
        "    #         label_ids,\n",
        "    #         predicted_labels)\n",
        "    #     auc = tf.metrics.auc(\n",
        "    #         label_ids,\n",
        "    #         predicted_labels)\n",
        "    #     recall = tf.metrics.recall(\n",
        "    #         label_ids,\n",
        "    #         predicted_labels)\n",
        "    #     precision = tf.metrics.precision(\n",
        "    #         label_ids,\n",
        "    #         predicted_labels) \n",
        "    #     true_pos = tf.metrics.true_positives(\n",
        "    #         label_ids,\n",
        "    #         predicted_labels)\n",
        "    #     true_neg = tf.metrics.true_negatives(\n",
        "    #         label_ids,\n",
        "    #         predicted_labels)   \n",
        "    #     false_pos = tf.metrics.false_positives(\n",
        "    #         label_ids,\n",
        "    #         predicted_labels)  \n",
        "    #     false_neg = tf.metrics.false_negatives(\n",
        "    #         label_ids,\n",
        "    #         predicted_labels)\n",
        "    #     return {\n",
        "    #         \"eval_accuracy\": accuracy,\n",
        "    #         \"f1_score\": f1_score,\n",
        "    #         \"auc\": auc,\n",
        "    #         \"precision\": precision,\n",
        "    #         \"recall\": recall,\n",
        "    #         \"true_positives\": true_pos,\n",
        "    #         \"true_negatives\": true_neg,\n",
        "    #         \"false_positives\": false_pos,\n",
        "    #         \"false_negatives\": false_neg\n",
        "    #     }\n",
        "\n",
        "    #   eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "    #   if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    #     return tf.estimator.EstimatorSpec(mode=mode,\n",
        "    #       loss=loss,\n",
        "    #       train_op=train_op)\n",
        "    #   else:\n",
        "    #       return tf.estimator.EstimatorSpec(mode=mode,\n",
        "    #         loss=loss,\n",
        "    #         eval_metric_ops=eval_metrics)\n",
        "    # else:\n",
        "    out_layer = create_model( input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "    \n",
        "    predictions = {\n",
        "              'outlayer': out_layer\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8"
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE =32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 5.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "SAVE_SUMMARY_STEPS = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHxaxU6VNdaa"
      },
      "source": [
        "binwid = 8000\n",
        "totalbins = int(len(traintest_features)/binwid)+1\n",
        "\n",
        "for binno in range (totalbins+1):\n",
        "  print(binno)\n",
        "  low, high = [binno*binwid, binno*binwid +binwid]\n",
        "  train_features = traintest_features[low:high]\n",
        "  print(low, high)\n",
        "  num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "  model_fn = model_fn_builder(  num_labels=len(label_list),\n",
        "   learning_rate=LEARNING_RATE,\n",
        "   num_train_steps=num_train_steps,\n",
        "   num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "  estimator = tf.estimator.Estimator( model_fn=model_fn,\n",
        "    # config=run_config,\n",
        "    params={\"batch_size\": BATCH_SIZE})\n",
        "  predict_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "  # train_input_fn = \n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  all_berts = []\n",
        "  for pre in predictions:\n",
        "    i = pre['outlayer']\n",
        "    # print(i.shape)\n",
        "    # break\n",
        "    all_berts.append(i)\n",
        "  all_berts = np.array(all_berts)\n",
        "  print(all_berts.shape)\n",
        "  np.save(r'./results/all_berts_vec_{}'.format(binno), all_berts)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvbE4oiIlJiJ"
      },
      "source": [
        "vecs = []\n",
        "for binno in range(26):\n",
        "  vec = np.load(r'./results/all_berts_vec_{}.npy'.format(binno))\n",
        "  print(vec.shape)\n",
        "  if len(vec)>0:\n",
        "    vecs.append(vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRFTRFdAn_Lq"
      },
      "source": [
        "all_vec = np.concatenate(vecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-auSj31oEOW"
      },
      "source": [
        "all_vec.shape\n",
        "np.save(r'./results/all_berts_vec', all_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_"
      },
      "source": [
        "# # Compute # train and warmup steps from batch size\n",
        "# topn = 20000\n",
        "# train_features = traintest_features[:topn]\n",
        "# num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "# num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gda59BvfoDpj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgcRCReqNVya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa"
      },
      "source": [
        "# # Specify outpit directory and number of checkpoint steps to save\n",
        "# run_config = tf.estimator.RunConfig(\n",
        "#     model_dir=OUTPUT_DIR,\n",
        "#     save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "#     save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v"
      },
      "source": [
        "# model_fn = model_fn_builder(\n",
        "#   num_labels=len(label_list),\n",
        "#   learning_rate=LEARNING_RATE,\n",
        "#   num_train_steps=num_train_steps,\n",
        "#   num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "# estimator = tf.estimator.Estimator( model_fn=model_fn,\n",
        "#   # config=run_config,\n",
        "#   params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOO3RfG1DYLo"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K"
      },
      "source": [
        "# Create an input function for prediction. drop_remainder = True for using TPUs.\n",
        "predict_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "# train_input_fn = \n",
        "predictions = estimator.predict(predict_input_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "364B1qnIV_Ut"
      },
      "source": [
        "all_berts = []\n",
        "for pre in predictions:\n",
        "  i = pre['outlayer']\n",
        "  # print(i.shape)\n",
        "  # break\n",
        "  all_berts.append(i)\n",
        "all_berts = np.array(all_berts)\n",
        "np.save(r'./results/all_berts_vec', all_berts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_TjUQFDBTrA"
      },
      "source": [
        "all_berts.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlzJ95HkNQUY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ-CFH5CNQm6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AshGLObNQp-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCjA2umXNQtS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fln2boiyNQwH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4j7Vgc3NQy7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-"
      },
      "source": [
        "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK"
      },
      "source": [
        "# print(f'Beginning Training!')\n",
        "# current_time = datetime.now()\n",
        "# estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "# print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3"
      },
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx"
      },
      "source": [
        "# test_input_fn = run_classifier.input_fn_builder(\n",
        "#     features=test_features,\n",
        "#     seq_length=MAX_SEQ_LENGTH,\n",
        "#     is_training=False,\n",
        "#     drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-"
      },
      "source": [
        "# a = estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKsULteiz1B"
      },
      "source": [
        "Now let's write code to make predictions on new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsrbTD2EJTVl"
      },
      "source": [
        "# def getPrediction(in_sentences):\n",
        "#   labels = [\"Negative\", \"Positive\"]\n",
        "#   input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "#   input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "#   predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "#   predictions = estimator.predict(predict_input_fn)\n",
        "#   return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-thbodgih_VJ"
      },
      "source": [
        "# pred_sentences = [\n",
        "#   \"That movie was absolutely awful\",\n",
        "#   \"The acting was a bit lacking\",\n",
        "#   \"The film was creative and surprising\",\n",
        "#   \"Absolutely fantastic!\"\n",
        "# ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrZmvZySKQTm"
      },
      "source": [
        "# predictions = getPrediction(pred_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzuTeBt8JzkV"
      },
      "source": [
        "# train on 90% non masked samples\n",
        "\n",
        "\n",
        "model_dir = OUTPUT_DIR +  + r'getbert/'\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "run_config = tf.estimator.RunConfig(  model_dir=model_dir, save_summary_steps=SAVE_SUMMARY_STEPS,save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "trainid, testid = sample(congress_flt, biased = False, notmask = 0.9)\n",
        "train_features = traintest_features[trainid]\n",
        "test_features = traintest_features[testid]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq72fMwoKOby"
      },
      "source": [
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "# continue\n",
        "model_fn = model_fn_builder(  num_labels=len(label_list),  learning_rate=LEARNING_RATE,  num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator( model_fn=model_fn, config=run_config,  params={\"batch_size\": BATCH_SIZE})\n",
        "train_input_fn = bert.run_classifier.input_fn_builder( features=train_features,  seq_length=MAX_SEQ_LENGTH, is_training=True,   drop_remainder=False)\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "test_input_fn = run_classifier.input_fn_builder(features=test_features,  seq_length=MAX_SEQ_LENGTH,    is_training=False,    drop_remainder=False)\n",
        "testresult = estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "acc = testresult['eval_accuracy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXkRiEBUqN3n"
      },
      "source": [
        "Voila! We have a sentiment classifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDnLiPWdBEYg"
      },
      "source": [
        "# start predict\n",
        "\n",
        "predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "\n",
        "\n",
        "predictions = np.array(list(estimator.predict(predict_gen)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mOrgRWFebGc"
      },
      "source": [
        "np.save(r'./congress_data/bert_vec_2009_20', predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}