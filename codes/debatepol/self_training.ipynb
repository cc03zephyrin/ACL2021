{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CDVWnOKtK3Yy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# iris = datasets.load_iris()\n",
    "# rng = np.random.RandomState(42)\n",
    "# random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\n",
    "# labels = np.copy(iris.target)\n",
    "# labels[random_unlabeled_points] = -1\n",
    "# label_prop_model.fit(iris.data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datapath =  r'D:\\projects\\congress\\processed\\training_vectors\\debatepol/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IrB8GAbHLok5"
   },
   "outputs": [],
   "source": [
    "\n",
    "ds_all = np.load(os.path.join(datapath, 'sentence_vectors_guns_nomoder.npy'))\n",
    "parties_all = np.load(os.path.join(datapath, 'stance_lib_binary_guns_nomoder.npy'))\n",
    "stance_all = np.load(os.path.join(datapath, 'stance_lib_7scale_guns_nomoder.npy'))\n",
    "thetas_hard_all = np.load(os.path.join(datapath, 'topics_hard_guns_nomoder.npy'))\n",
    "As_all = np.load(os.path.join(datapath, 'author_dummies_guns_nomoder.npy')) \n",
    "theta_categ= np.argmax(thetas_hard_all, axis = -1)\n",
    "ds_all = np.float32(ds_all)\n",
    "thetas_all = thetas_hard_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gL_hzD211rre",
    "outputId": "882e85b3-f34a-4f26-87da-434585c32138"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52675, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "k-ji6tuxRrod"
   },
   "outputs": [],
   "source": [
    "def sample(parties_all, theta_categ, stance_all,biased = True, notmask = 0.8, extreme = 5):\n",
    "\n",
    "    totalsamp = len(parties_all)\n",
    "\n",
    "    if not biased: \n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size= 1- notmask)\n",
    "        train_id, test_id = next(sss.split(theta_categ, parties_all))\n",
    "    else:\n",
    "        p = np.array([(4- i)**extreme  if i <4 else (i -4)**extreme for i in stance_all])\n",
    "        p = p / np.sum(p)\n",
    "        train_id = np.random.choice(np.arange(totalsamp), int(totalsamp * notmask), replace = False, p = p)\n",
    "        \n",
    "#         orderid = np.argsort(stance_all)\n",
    "#         tbn = int(len(parties_all) * notmask/2)\n",
    "#         train_id = np.concatenate([orderid[:tbn], orderid[-tbn:]])\n",
    "        \n",
    "        test_id = [i for i in range(totalsamp) if i not in train_id]\n",
    "\n",
    "    return train_id, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAvW6-q0-kp2",
    "outputId": "015a5543-ad8f-4148-a61b-9407bfcbe85e"
   },
   "outputs": [],
   "source": [
    "trainid, testid = sample(parties_all, theta_categ, stance_all,biased = True, notmask =0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n4slL2XB-5y0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52675"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainid) + len(testid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbMa60SJTfo9",
    "outputId": "1d455db5-3405-4b7d-c240-7a2d7fbc6abf"
   },
   "outputs": [],
   "source": [
    "# rounds = 6\n",
    "# notmasks = [0.01, 0.03, 0.05, 0.08, 0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# # notmasks = [0.8]\n",
    "# topn = 500000\n",
    "# model_name = 'label_spreading'\n",
    "# label_prop_model = LabelSpreading(kernel= 'knn', n_neighbors=10)\n",
    "# for biased in [True, False]:\n",
    "#   bia = 'bias'\n",
    "#   if not biased:\n",
    "#     bia = 'unbias'\n",
    "#   f = open( r'./results/SKSSL/{}_{}.txt'.format(model_name, bia), 'w')\n",
    "#   for notmask in notmasks:\n",
    "#     version = \"{}_{}_{}\".format(model_name, notmask, bia)\n",
    "#     print(version)\n",
    "#     if biased:\n",
    "# #       continue\n",
    "#       trainid, testid = sample(labels, topics, biased = biased, notmask = notmask)\n",
    "\n",
    "#       ys = np.copy(labels)\n",
    "#       ys[testid] = -1\n",
    "\n",
    "#       Xs = np.copy(sentvec_flt)\n",
    "#       Xidx = np.arange(len(Xs))\n",
    "      \n",
    "#       np.random.shuffle(Xidx)\n",
    "#       Xs = Xs[Xidx]\n",
    "#       ys = ys[Xidx]\n",
    "#       print(ys[:100])\n",
    "#       label_prop_model.fit(Xs[:topn], ys[:topn])\n",
    "#       Xt = sentvec_flt[testid]\n",
    "#       yt = labels[testid]\n",
    "#       y_pred = label_prop_model.predict(Xt)\n",
    "#       acc = np.mean(y_pred == yt)\n",
    "#       print(acc)\n",
    "#       f.write(version)\n",
    "#       f.write('\\t'+str(acc))\n",
    "#       f.write('\\n')\n",
    "#     else:\n",
    "#       accs = []\n",
    "#       for __ in range(rounds):\n",
    "#         trainid, testid = sample(labels, topics, biased = biased, notmask = notmask)\n",
    "#         ys = np.copy(labels)\n",
    "#         ys[testid] = -1\n",
    "        \n",
    "#         Xs = np.copy(sentvec_flt)\n",
    "#         Xidx = np.arange(len(Xs))\n",
    "      \n",
    "#         np.random.shuffle(Xidx)\n",
    "#         Xs = Xs[Xidx]\n",
    "#         ys = ys[Xidx]\n",
    "#         print(ys[:100])\n",
    "#         label_prop_model.fit(Xs[:topn], ys[:topn])\n",
    "#         Xt = sentvec_flt[testid]\n",
    "#         yt = labels[testid]\n",
    "#         y_pred = label_prop_model.predict(Xt)\n",
    "#         acc = np.mean(y_pred == yt)\n",
    "#         print(acc)\n",
    "#         accs.append(acc)\n",
    "#       f.write(version)\n",
    "#       for ac in accs:\n",
    "#         f.write('\\t'+str(ac))\n",
    "#       f.write('\\n')\n",
    "#   f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_party_membership(s_pred, y, As):\n",
    "    # filter out nan\n",
    "    \n",
    "    fin = np.isfinite(s_pred)\n",
    "    s_pred = s_pred[fin]\n",
    "    y = y[fin]\n",
    "    As = As[fin]\n",
    "    nonzero_inx = (np.sum(As.T, axis = -1) !=0)\n",
    "    ave_slants = (np.matmul(As.T, s_pred)/(np.sum(As.T, axis = -1) + 0.000001))[nonzero_inx]\n",
    "    y_ = np.matmul(As.T, y) >0 # this is party label per person, but those with 0 speech be 0\n",
    "    print(\"test slant\", ave_slants.mean(), ave_slants[:5])\n",
    "    y_true = y_[nonzero_inx]\n",
    "    y_pred = ave_slants>0\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kjsNJMUUTgBW"
   },
   "outputs": [],
   "source": [
    "# self training\n",
    "extreme = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_training_0.01_bias\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c200b3d471de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXidx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mself_training_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0myt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparties_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\sklearn\\semi_supervised\\_self_training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    204\u001b[0m             self.base_estimator_.fit(\n\u001b[0;32m    205\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msafe_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                 self.transduction_[has_label])\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;31m# Validate the fitted estimator since `predict_proba` can be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    391\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[1;32m--> 393\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    167\u001b[0m                                                         indices=indices)\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    905\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 907\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    908\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    392\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rounds = 7\n",
    "notmasks = [0.01, 0.03, 0.05, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# notmasks = [0.8]\n",
    "topn = 5000000\n",
    "model_name = 'self_training'\n",
    "\n",
    "\n",
    "# notmasks = [0.01]\n",
    "# topn = 5000\n",
    "# rounds = 2\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "self_training_model = SelfTrainingClassifier(clf)\n",
    "# for biased in [True, False]:\n",
    "for biased in [True, False]:\n",
    "  bia = 'bias'\n",
    "  if not biased:\n",
    "    bia = 'unbias'\n",
    "  f = open( r'D:\\projects\\congress\\results\\debatepol\\train\\SSL/{}_{}.txt'.format(model_name, bia), 'w')\n",
    "  f.write('\\taccs\\tpccs\\n')\n",
    "  for notmask in notmasks:\n",
    "    version = \"{}_{}_{}\".format(model_name, notmask, bia)\n",
    "    print(version)\n",
    "    \n",
    "    if biased:\n",
    "      accs = []\n",
    "      pccs = []\n",
    "      for __ in range(rounds):\n",
    "        trainid, testid = sample(parties_all, theta_categ, stance_all, biased = biased, notmask = notmask, extreme = extreme)\n",
    "        ys = np.copy(parties_all)\n",
    "        ys[testid] = -1\n",
    "\n",
    "        Xs = np.copy(ds_all)\n",
    "        Xidx = np.arange(len(Xs))\n",
    "      \n",
    "        np.random.shuffle(Xidx)\n",
    "        Xs = Xs[Xidx]\n",
    "        ys = ys[Xidx]\n",
    "        print(ys[:100])\n",
    "        self_training_model.fit(Xs[:topn], ys[:topn])\n",
    "        Xt = ds_all[testid]\n",
    "        yt = parties_all[testid]\n",
    "        At = As_all[testid]\n",
    "        y_pred = self_training_model.predict(Xt)\n",
    "        proba_pred = self_training_model.predict_proba(Xt)\n",
    "        s_pred = np.array([np.log(i[1]*10001)- np.log(i[0]*1000) for i in proba_pred])\n",
    "\n",
    "        acc = np.mean(y_pred == yt)\n",
    "        accs.append(acc)\n",
    "        \n",
    "        pcc = predict_party_membership(s_pred, yt, At)\n",
    "        pccs.append(pcc)\n",
    "        \n",
    "        \n",
    "      f.write(version)\n",
    "\n",
    "      f.write('\\t' + str(accs))\n",
    "      f.write('\\t' + str(pccs))\n",
    "      f.write('\\n')\n",
    "    else:\n",
    "      accs = []\n",
    "      pccs = []\n",
    "      for __ in range(rounds):\n",
    "        trainid, testid = sample(parties_all, theta_categ, stance_all, biased = biased, notmask = notmask, extreme = extreme)\n",
    "        ys = np.copy(parties_all)\n",
    "        ys[testid] = -1\n",
    "\n",
    "        Xs = np.copy(ds_all)\n",
    "        Xidx = np.arange(len(Xs))\n",
    "      \n",
    "        np.random.shuffle(Xidx)\n",
    "        Xs = Xs[Xidx]\n",
    "        ys = ys[Xidx]\n",
    "        print(ys[:100])\n",
    "        self_training_model.fit(Xs[:topn], ys[:topn])\n",
    "        Xt = ds_all[testid]\n",
    "        yt = parties_all[testid]\n",
    "        At = As_all[testid]\n",
    "        y_pred = self_training_model.predict(Xt)\n",
    "        proba_pred = self_training_model.predict_proba(Xt)\n",
    "        s_pred = np.array([np.log(i[1]*1000)- np.log(i[0]*1000) for i in proba_pred])\n",
    "\n",
    "        acc = np.mean(y_pred == yt)\n",
    "        accs.append(acc)\n",
    "        \n",
    "        pcc = predict_party_membership(s_pred, yt, At)\n",
    "        pccs.append(pcc)\n",
    "        \n",
    "        \n",
    "      f.write(version)\n",
    "\n",
    "      f.write('\\t' + str(accs))\n",
    "      f.write('\\t' + str(pccs))\n",
    "      f.write('\\n')\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_training_boosting_0.01_bias\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.6525330577119072 [-2.32235263  1.51016004  5.77772731 -0.90964582 -1.06624265]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.8010347504810774 [1.82613179 0.54872766 1.83041794 1.42367031 3.54231028]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.8755839989059188 [ 0.23477053  5.23321411 -0.10571222 -0.13298374  2.60277049]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.04001764648743245 [ 0.12261547 -0.02388291  4.3609529  -1.09758986  1.09326953]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 1.6158547756266293 [ 0.28873665  2.65489963 -1.35173075  3.25403572  1.88649414]\n",
      "self_training_boosting_0.03_bias\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.6875741176352558 [-1.05734445  2.39642711 -2.20978951  2.61785806  2.42180492]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.35420035764056185 [ 0.88402184  2.24835504 -0.57898828 -2.23978218  1.84029166]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  0 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1  1 -1 -1]\n",
      "test slant -0.01987677812132245 [-1.48524486  2.58998946  1.63353489 -4.03638721 -0.77820058]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.04711055464952683 [ 0.24140301 -0.10076781  0.40802208 -3.04310394 -1.23151443]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1  1 -1 -1]\n",
      "test slant 0.6883763182464804 [ 1.94645484  2.61817342  4.69828186 -1.30362774 -0.61944452]\n",
      "self_training_boosting_0.05_bias\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.4704090437962152 [-1.70853165  3.04938583 -1.00547676 -2.58001626  0.18192625]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.6988151579123525 [-0.39093064  3.67388349  2.58520814 -1.28223876  2.70276133]\n",
      "[-1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1  1 -1 -1 -1 -1 -1  0\n",
      "  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.8870122104200893 [ 1.06190737  0.31437875  3.29940223 -1.68733261  0.92606144]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1  0 -1 -1  1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.5944123770913462 [ 0.75713623  4.86917876  2.59067798 -1.39785225 -0.21605843]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0\n",
      " -1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1  1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.28987574875601657 [ 0.36424601  0.69297906 -1.17332558 -1.34921271  2.2981523 ]\n",
      "self_training_boosting_0.08_bias\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1  0  0 -1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.8295513064732764 [-0.70274707  3.690272    4.42268572  0.47686324  1.96285143]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1  1  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1 -1 -1 -1]\n",
      "test slant 0.6735038653710778 [-0.16235952  2.7361365   3.04057967  0.15602988 -0.34513592]\n",
      "[-1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1  0  0 -1\n",
      " -1 -1 -1 -1 -1 -1 -1  0 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1  1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1  0\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.5215680571528984 [ 1.390929    3.5787961   6.02816273 -1.04557926 -0.55749826]\n",
      "[-1 -1 -1  0 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1\n",
      " -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.6164966849393085 [-1.79180785  4.13058976  4.56913747  0.31178714  0.32796624]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  0 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  0 -1  0 -1 -1 -1 -1 -1  0 -1 -1  0 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1\n",
      " -1  0 -1 -1]\n",
      "test slant 0.7025497045557935 [ 0.37183104 -1.38858247 -1.59407171  1.129193    0.59053721]\n",
      "self_training_boosting_0.1_bias\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1  1 -1  0 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1  1 -1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0\n",
      " -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test slant 1.163370998164613 [-2.19645658  3.78236317  3.79971014  0.04190505  2.56275709]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 1.099105214151821 [ 0.93631428  1.3264587   2.68394645 -0.55962863  0.0096224 ]\n",
      "[ 1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  0  0 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 0.656033794607162 [-1.53317627 -0.2256957   2.47345028  1.7136183  -0.10347767]\n",
      "[-1  1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  0 -1  0 -1 -1  0  1 -1 -1 -1 -1  0 -1 -1 -1  0 -1  1\n",
      "  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1\n",
      " -1 -1  1 -1]\n",
      "test slant 0.669865848312597 [ 0.30415118  2.08450201  4.87764434 -1.25154131  0.13624952]\n",
      "[-1  0 -1 -1  0 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1\n",
      " -1 -1  1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  0]\n",
      "test slant 0.7499775284238478 [ 1.05216867  0.35416133  3.01721198 -0.46351865  1.74621064]\n",
      "self_training_boosting_0.2_bias\n",
      "[ 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1\n",
      " -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1\n",
      " -1  0 -1 -1 -1 -1 -1 -1  0  1 -1  1 -1 -1 -1 -1  1 -1  0 -1 -1 -1 -1  1\n",
      " -1 -1 -1 -1  1 -1 -1 -1 -1 -1  1 -1  1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant 1.1738171001490627 [ 0.11782082  4.2633478  -0.61217814  0.57619028  1.01174393]\n",
      "[-1 -1 -1 -1  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  0  1  1\n",
      " -1 -1 -1  0 -1 -1  0 -1  1 -1 -1 -1 -1  0  0 -1 -1 -1  1 -1  1 -1  0 -1\n",
      "  0 -1 -1 -1 -1 -1  1 -1 -1  0 -1 -1 -1 -1 -1  0 -1 -1 -1  1 -1 -1 -1 -1\n",
      " -1  1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  0\n",
      " -1 -1 -1  1]\n",
      "test slant 1.1877489564171513 [ 2.58221524  1.20345184 -0.09850381 -1.36024799  2.00084294]\n",
      "[ 0 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1  1  1\n",
      " -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1  0  0 -1 -1 -1 -1  1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1\n",
      " -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  0 -1  0  1 -1 -1 -1 -1 -1 -1 -1  1\n",
      " -1 -1 -1 -1]\n",
      "test slant 1.2968115590074554 [0.69961663 3.08727356 2.85639273 1.27144861 2.18130836]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1  0  0 -1 -1 -1  0  0  1  0 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  0 -1 -1 -1 -1 -1 -1  0 -1  1  0 -1 -1 -1 -1 -1 -1 -1 -1  1 -1\n",
      " -1  0  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1\n",
      "  1  0  1 -1]\n",
      "test slant 1.1537392597860798 [-0.17184422  3.29017185  0.9726545   1.3515348   0.77598607]\n",
      "[-1 -1 -1  1 -1 -1  1 -1 -1 -1 -1  0 -1  1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1\n",
      " -1 -1  1 -1 -1 -1  0 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  1 -1\n",
      " -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  1 -1 -1 -1  1 -1 -1  1 -1 -1 -1 -1\n",
      "  1 -1  0 -1]\n",
      "test slant 1.1171691642848567 [-1.03087261  1.88920651  0.81407359  0.51038214  2.54429084]\n",
      "self_training_boosting_0.4_bias\n",
      "[ 0  0  0 -1  0 -1  1  1 -1 -1  0  1  0  1  0  1 -1 -1  1 -1 -1 -1 -1 -1\n",
      "  0 -1 -1  1  0  0 -1  0 -1 -1  0 -1  0  0 -1 -1  0  1 -1 -1  1  0  0 -1\n",
      "  0 -1 -1 -1  1 -1 -1 -1 -1  0  0 -1  0 -1  0  0  0 -1  0 -1  1  0  0 -1\n",
      "  0 -1 -1  1  0  0 -1 -1 -1  0  1 -1 -1 -1  1 -1  1 -1 -1 -1 -1 -1  0  0\n",
      " -1 -1 -1 -1]\n",
      "test slant 1.882565991194419 [4.85372407 3.06353922 0.41676598 0.45582795 1.77972467]\n",
      "[-1  1  1  0  1 -1 -1  0  1  1  0  0 -1 -1 -1 -1 -1 -1 -1 -1  0 -1  1  0\n",
      "  0 -1  1 -1 -1 -1  1  0 -1 -1  1 -1 -1  1  0 -1  0 -1 -1 -1 -1 -1 -1  0\n",
      " -1  0 -1 -1 -1 -1  1 -1  1  0  0  0  1  0 -1  1 -1  1  0  0  0 -1  1  1\n",
      "  0  0  1 -1  1 -1 -1 -1 -1  0  0 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  1  0 -1\n",
      "  1  1 -1  1]\n",
      "test slant 1.8303815715068337 [-0.51297323  0.574767    2.55927864  1.63567056  1.18068814]\n",
      "[-1  1  0 -1  1  0  1 -1 -1 -1  0  1  0 -1 -1  1 -1  1 -1 -1 -1 -1  0 -1\n",
      "  1 -1 -1  0 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1  1 -1  0  0 -1 -1  1 -1 -1\n",
      "  1  1 -1 -1  1 -1 -1  0 -1 -1  1 -1 -1 -1  0  1  0 -1 -1  0 -1 -1  0 -1\n",
      "  0 -1 -1  1 -1  1  0  0 -1 -1 -1 -1  1 -1  0 -1  0 -1  0  1 -1  0  0 -1\n",
      "  1 -1  1  1]\n",
      "test slant 1.728520477679463 [ 1.41783078 -0.01749067  2.52024796  1.72339255  2.02873234]\n",
      "[ 0  1  1 -1 -1  1 -1 -1  0  0  0  0  0  0  0  0 -1 -1 -1  0 -1 -1  1 -1\n",
      " -1  0  0  0 -1 -1  0 -1  1 -1  1 -1  1 -1  0  1 -1  0  0 -1 -1  1 -1  1\n",
      "  1 -1 -1  0 -1 -1  0 -1  0 -1  1 -1  1  1 -1 -1 -1  1  1 -1 -1  0 -1  0\n",
      " -1 -1  1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1  1  0  0  0 -1 -1 -1  0\n",
      " -1 -1  0 -1]\n",
      "test slant 1.8515220008609796 [1.76450617 3.4682164  2.49881321 1.63624049 1.94136968]\n",
      "[-1  1 -1  0  0 -1 -1 -1  1 -1 -1 -1  1 -1 -1  0 -1  0  0  1  0 -1 -1 -1\n",
      "  1  1  0 -1 -1 -1 -1 -1 -1 -1  0 -1  0  0  0 -1  0  0 -1 -1  0 -1 -1 -1\n",
      "  0  0  0 -1 -1 -1 -1 -1  0 -1 -1  0 -1 -1 -1  1 -1  0  1 -1  1  1  0 -1\n",
      " -1 -1  0 -1  1  1  0 -1 -1  0 -1  1  1 -1  1  1 -1  0  1 -1 -1 -1 -1 -1\n",
      " -1 -1  0  0]\n",
      "test slant 1.8892842382903425 [1.65082472 0.82977759 1.88528178 1.77099202 1.85917598]\n",
      "self_training_boosting_0.6_bias\n",
      "[ 1  0 -1  0 -1  1 -1  0 -1  0  0 -1  1 -1  0  1  1  0 -1 -1 -1 -1  0  1\n",
      "  1  0  0  0  0  1  0  1  0  1 -1  1  0 -1  0 -1 -1  0  1  1  0 -1 -1  0\n",
      "  0 -1 -1 -1 -1  0  1 -1 -1 -1  1  0  1  0  0 -1  1 -1  0  0 -1 -1  0  1\n",
      "  0 -1  1 -1 -1  1  0  0 -1 -1 -1 -1  1  1 -1  0  1 -1  0  1 -1  1  1  1\n",
      "  0  0 -1  1]\n",
      "test slant 2.018187555292174 [0.88406341 2.63200014 1.91621479 2.37784123 2.56431682]\n",
      "[-1 -1  0  1  0 -1  0  0  0 -1 -1 -1  0 -1  0  0 -1  1 -1 -1 -1  1  1  0\n",
      " -1  0 -1 -1 -1 -1  0  1  1 -1  1  1 -1 -1  0  0 -1  1 -1  0  1  1  0  0\n",
      " -1  0  0 -1 -1  1  0 -1  1  1  0 -1  1 -1 -1  1  1  0  0  1 -1  1  0  1\n",
      " -1  1 -1 -1  1 -1 -1  0  1 -1 -1 -1  1 -1 -1 -1 -1  1  0 -1  0  1  0 -1\n",
      "  0 -1  0 -1]\n",
      "test slant 1.9971818046391818 [1.64370925 1.20657768 2.75203644 1.96415603 1.55961175]\n",
      "[ 0  1  1  1  0  1  0  1  1  1 -1 -1  1 -1  0  0 -1 -1  0  0  1  1  1 -1\n",
      "  0  1  0 -1 -1 -1  0 -1 -1  0  0  1  1  1 -1  1  1  1  1 -1  0  1  1 -1\n",
      "  0  0  0 -1  1  1 -1  0 -1 -1 -1  1  1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1  0  1  0  0  0  0 -1 -1 -1 -1  1 -1  1  1 -1  0 -1  0 -1  0  0 -1  0\n",
      " -1  1 -1  0]\n",
      "test slant 2.005685498866819 [1.69820215 1.91374159 2.50110853 5.5581703  2.66493318]\n",
      "[ 0  1  1 -1  0 -1 -1 -1  0  0  1  0  0  1  1 -1  0  0 -1 -1  0 -1  0 -1\n",
      "  0 -1  0 -1 -1 -1 -1  0  1  1 -1  1  0  0  1  1 -1  0  1 -1  1 -1 -1  1\n",
      "  1 -1  0  0  1  1  1  1  1 -1  1  0  1  1  0  0  1  0 -1 -1  0 -1 -1 -1\n",
      "  1  1  1  0  0  1 -1  1  1 -1 -1 -1 -1  0  0  1 -1  1  1  0  1  0 -1  0\n",
      " -1  1  1  0]\n",
      "test slant 2.0674390678553887 [3.17438822 3.78205456 1.85011775 1.98750496 2.35836393]\n",
      "[ 1  1 -1  1 -1  1  1 -1  0  1  1  0  1  1  0  0 -1 -1 -1  1  0 -1  0  1\n",
      " -1 -1  1  1 -1  1 -1 -1  1  1 -1  1  0  0  0 -1  0 -1  1  0  0  0 -1  0\n",
      " -1  1 -1  1 -1  1  1  0  1 -1  0 -1 -1 -1 -1  1  0 -1 -1  0 -1  0 -1  0\n",
      " -1 -1 -1  1 -1  1  0  0  0  0 -1  1  1  0  1 -1  0 -1 -1 -1  1  1 -1 -1\n",
      "  1  0  0  1]\n",
      "test slant 2.0871322002177304 [3.25442273 1.66856965 2.12417167 1.86759084 2.43337937]\n",
      "self_training_boosting_0.8_bias\n",
      "[-1  0 -1  0  1  1  0  1  0  0  0 -1  1 -1  0  1  0  1  1  0  1  1 -1  1\n",
      "  1  1  1  0 -1  0  0  1  1  0  1  0  1  1  1  1  0  1  1  1  0  0  1  1\n",
      "  1  0  0  0  1  1  0  0  1  0  1  0  1  0  1  0  0  0  0  1  1  0  0  0\n",
      "  0  1 -1 -1 -1  0 -1  0  1  0  1  1  0  0  1  0  1  0  0 -1 -1 -1  0 -1\n",
      " -1 -1 -1  0]\n",
      "test slant 2.1037172697411566 [4.64342518 2.08118463 2.35749537 0.67114158 1.99749043]\n",
      "[-1  0  1  1  1  1  0  1  0  0 -1 -1  1  0  0  1  0  1  0  1  1  1 -1  1\n",
      "  0  1  0  0  1  0  1  1  1 -1  0  1  0  0  1  0  1  0  1  0  0 -1  1  0\n",
      "  1 -1  1  0  1 -1  1 -1  1  0  0  0  1  1  1  1  0  0 -1  1  0  0  0  1\n",
      " -1  1  1  0  1  1 -1  1  0  1 -1  1  0 -1 -1  0  1  0  0  0  1  1  0 -1\n",
      "  1 -1 -1  0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test slant 2.1305237784829703 [2.08650554 2.37954838 2.52009281 1.43825389 2.04143847]\n",
      "[-1  1  0 -1 -1  0  0  1  1  0 -1  0  0  0  1  0  1  1 -1  1  1  1  0 -1\n",
      "  0  1  0  1  0  0  0  1  1 -1 -1  0  0  1  1 -1  1  0  0 -1  0  0 -1  0\n",
      "  1 -1  1 -1  0  1  1  1  1  1  0  1 -1  0  1  1  1  1  0  0  1  0  1  0\n",
      " -1  1  0  0  0  0  1  0  0  1 -1  0  1  1  0  0  0  0  0 -1  0  1  1  0\n",
      "  0  0  1 -1]\n",
      "test slant 2.1420990112932428 [1.92944675 2.03825392 2.44185946 2.26927785 1.88521894]\n",
      "[-1  0 -1  0  1  0  0  1  1  0  0  0  0  0  1  0  1  1  0  0 -1  0  0  0\n",
      "  0  0  1  0  0  0 -1 -1  0  1  0  1  0 -1  0  0  0  0 -1  0  0  1 -1  1\n",
      "  0  1  0  0  1  0  1  0 -1  1  0  1  1  1  0  1  1 -1  1  1  1  0  1  1\n",
      "  0  1  1 -1  1  0  1  1  1  1  0  1  1  0  0  1  1 -1  1  0  1 -1  0 -1\n",
      "  0  1 -1  0]\n",
      "test slant 2.037324524118592 [1.93617268 1.79065735 2.43664703 1.94507553 1.9427733 ]\n",
      "[-1 -1  0  1  1  1  0  1  0  0  0  0  0  0  1  1  1  1  1  0  1  0  1  1\n",
      "  0  1  0  1  0  1  1  0  0  0 -1 -1  1 -1  0  0  1  0 -1  1  0  0  0  1\n",
      "  1  0  0  1  0  1 -1 -1  1  0  1  1  0  1  1 -1  1  0  0  0  0  0  0  1\n",
      "  0 -1  1 -1  0  1  0  1 -1  1 -1  0  0 -1 -1  1  0  1  1  0  0  0  0  1\n",
      " -1  0  0  0]\n",
      "test slant 2.128060235566373 [1.86204903 2.5076187  2.42920722 2.86424624 1.90716592]\n",
      "self_training_boosting_0.01_unbias\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.9352709388427446 [-0.03706032 -1.5108826  -1.48061582 -3.2163668  -0.20373116]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.6670850041769144 [-1.30423656  0.71262486  0.63277764 -1.87813886 -0.39733946]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.3003380929450028 [-1.05204308 -3.23546236 -3.67761094 -4.89867514 -1.82062806]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.8375651671581723 [-0.06523826 -0.62771614  0.83837691 -3.21473154  1.14171505]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.826565281088172 [-1.34995821e+00 -1.48514735e+00  1.36394456e-03 -1.14520827e+00\n",
      " -1.80017136e+00]\n",
      "self_training_boosting_0.03_unbias\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.8237155105432691 [-1.19838726 -1.41738457 -1.8320595  -3.75398833 -1.73305831]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.044110411371729 [-2.59256058 -2.66556934 -2.15428154 -3.07792026 -2.3295264 ]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.3453493144477358 [ 1.0543357   0.08337058 -0.99029674 -3.15646006 -1.07617077]\n",
      "[-1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.2857391008234236 [-0.78776196  3.89177065  0.54306932 -3.44359151 -1.16346111]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.3435606238881845 [-2.40966605  0.91588639  0.76127481 -2.60165863 -2.14802306]\n",
      "self_training_boosting_0.05_unbias\n",
      "[-1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.1708569608979247 [-0.89832774 -1.3122296   1.78944095 -3.00974994 -0.94389323]\n",
      "[-1 -1 -1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.1310167641054318 [-0.94524271 -3.25006302 -3.20916555 -2.57590111 -0.60300226]\n",
      "[-1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1  0 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.0222726683279943 [-1.29409625 -3.388922   -0.03721292 -1.4587791  -2.04401445]\n",
      "[-1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1  0 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.3139607547028194 [-1.51739896 -3.66629558 -5.42514039 -3.02743113 -2.23899621]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.2612778642990121 [ 1.27296819 -1.98207715 -0.72361243 -2.29184512 -2.35159276]\n",
      "self_training_boosting_0.08_unbias\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1 -1  0 -1 -1 -1\n",
      " -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1 -1 -1  1 -1  0\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1\n",
      " -1  0 -1 -1]\n",
      "test slant -0.9909389068529147 [ 0.26191398  0.81890849  0.18032761 -3.08876611 -0.21353134]\n",
      "[ 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1  1  0 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  0]\n",
      "test slant -1.1545985179818588 [ 0.15779075  4.17879997 -2.01086826 -2.00721385 -0.85052427]\n",
      "[ 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
      "  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  0  1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1\n",
      " -1 -1 -1 -1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test slant -1.3262581739596682 [-0.497558    1.80052258  0.39789682 -2.37647982  0.41340222]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.75747397771786 [ 2.62987457 -2.46907256 -1.52160837 -2.3881604   0.37852183]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1  1 -1 -1 -1  0 -1  0 -1 -1 -1  0 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.9559938132650518 [-0.49531895 -3.50749154 -0.28889092 -1.59933538 -1.13500525]\n",
      "self_training_boosting_0.1_unbias\n",
      "[-1 -1 -1 -1  0 -1 -1  1  0 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1  0 -1 -1  1  0 -1 -1 -1 -1  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  0 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.9959474096490593 [-2.1127206   0.56802083 -4.28129969 -1.91600534 -1.31429342]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.9457262614391638 [ 0.78644315 -1.2555231  -0.90846625 -0.98339979 -0.8172905 ]\n",
      "[-1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  0  1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  1 -1  1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1  1 -1 -1]\n",
      "test slant -0.8748076844873554 [-0.83055593 -2.58240597 -0.78714133 -4.47937826 -0.99094554]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1\n",
      "  1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  0 -1 -1 -1 -1  1 -1 -1 -1 -1  1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.3503741600544608 [ 0.60489927 -0.65002563 -1.37103438 -1.19220614 -2.00656176]\n",
      "[-1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1  1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1  1\n",
      " -1 -1 -1 -1]\n",
      "test slant -1.016319958893789 [-0.24118877 -0.94315832 -1.91758817 -0.42666105  0.06815848]\n",
      "self_training_boosting_0.2_unbias\n",
      "[-1  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1  0 -1 -1 -1 -1 -1\n",
      " -1  0 -1 -1 -1 -1 -1 -1 -1 -1  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1  0 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  0 -1 -1 -1 -1 -1  1 -1 -1 -1  1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.8577419097787097 [-1.16601292 -0.70509921 -0.34917017 -0.31418013 -0.19520929]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  0  1  0  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  0 -1 -1  1  0 -1  0 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0 -1  0\n",
      " -1 -1  0 -1]\n",
      "test slant -0.9201559132159799 [ 0.53303216 -1.4240002  -2.01787298 -1.67252111 -1.28778088]\n",
      "[-1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  0 -1  1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1  1 -1 -1  0 -1  0  0 -1 -1\n",
      " -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1\n",
      " -1  1  0  0]\n",
      "test slant -0.904205558909316 [ 0.4738412  -0.54808543 -1.24972616 -2.86813028 -0.48265383]\n",
      "[-1 -1  0 -1 -1 -1 -1 -1 -1 -1  1  1 -1 -1  0 -1 -1  1 -1  0 -1 -1 -1  0\n",
      " -1  0  1  1 -1 -1 -1 -1  1 -1  0 -1 -1  0  0 -1 -1 -1 -1  0 -1  1 -1  1\n",
      "  1 -1 -1 -1  0 -1 -1  0  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  1 -1 -1 -1 -1  1  1 -1 -1  0  1 -1 -1 -1 -1  0 -1 -1  0 -1 -1 -1\n",
      " -1  1 -1 -1]\n",
      "test slant -0.740090811081208 [-1.03010692 -1.72662816 -2.44131864 -1.29321332 -2.46912879]\n",
      "[-1 -1 -1 -1 -1 -1 -1  0 -1 -1  1 -1  1 -1  0 -1 -1 -1 -1 -1  0 -1  0 -1\n",
      " -1  0 -1  0 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1  0\n",
      " -1 -1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  0 -1  0  1 -1 -1 -1  1 -1 -1 -1 -1 -1  0 -1  1 -1 -1 -1 -1 -1\n",
      "  0 -1 -1 -1]\n",
      "test slant -0.6780334961959675 [-0.65379852  0.20605388 -1.36397211 -1.98977869 -0.36751628]\n",
      "self_training_boosting_0.4_unbias\n",
      "[-1  0 -1  0 -1 -1 -1  1 -1 -1 -1 -1  0 -1 -1 -1 -1  0  0 -1  1  0 -1 -1\n",
      " -1  0  0 -1 -1 -1  0  0 -1  1 -1  1  1 -1 -1 -1 -1 -1 -1  0 -1 -1  1  1\n",
      " -1  0  0  1  1  0 -1  0 -1  1  1 -1 -1 -1  0  0  1 -1 -1  0 -1  0 -1  0\n",
      "  0 -1 -1  1  0  1 -1 -1  0 -1 -1 -1  1  0 -1  0  0 -1 -1 -1 -1  0 -1  1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.4920772692133598 [ 0.55151252 -1.14624462 -2.02058433 -0.82394101 -0.5375132 ]\n",
      "[-1 -1 -1  1 -1 -1 -1 -1  0 -1  0  0 -1  0 -1 -1 -1  0 -1 -1 -1 -1  0  1\n",
      " -1 -1 -1 -1 -1  1  0 -1  1 -1 -1 -1  0  1  0 -1  0 -1  0 -1 -1 -1 -1 -1\n",
      " -1 -1  1 -1  0 -1 -1  0 -1  1 -1 -1  1  1 -1  0  1 -1  1 -1 -1 -1  0  0\n",
      " -1  0  0 -1 -1  0 -1 -1 -1  0  1  1  0 -1 -1  1 -1 -1 -1  1 -1 -1  0  0\n",
      "  0 -1  0  0]\n",
      "test slant -0.4710895594044709 [ 0.31302434  1.00177536 -1.50743228 -1.60068674  0.35560323]\n",
      "[ 0 -1 -1  0  0  0  0 -1 -1  1 -1 -1  1  0 -1 -1  1  0 -1  0 -1  1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1 -1 -1  0  0  0 -1 -1  0 -1 -1 -1  1 -1\n",
      " -1 -1 -1 -1  0  1 -1 -1 -1 -1  0 -1  1 -1  1 -1 -1 -1 -1  1 -1  1 -1 -1\n",
      "  0 -1  0 -1  1 -1  1 -1 -1 -1  0  0  1  0 -1  0 -1 -1  0  1 -1 -1  0 -1\n",
      " -1  0 -1  0]\n",
      "test slant -0.47546521363729044 [ 0.64705986 -0.86476609 -0.85924477 -0.71344274 -1.14520877]\n",
      "[ 1  1 -1 -1 -1  1 -1  1  0 -1  0  0 -1  0  0 -1 -1 -1 -1  0 -1 -1 -1 -1\n",
      "  0  0  1 -1 -1 -1  1 -1 -1  1 -1 -1  0  1 -1 -1  0 -1 -1 -1  0 -1 -1 -1\n",
      " -1 -1 -1  1  1  0 -1  0 -1  0 -1 -1 -1  0 -1  1  0  0 -1 -1 -1  1 -1 -1\n",
      " -1  0  0 -1  0 -1 -1 -1  0  1  0 -1 -1 -1 -1  0 -1  0  1 -1 -1 -1  1 -1\n",
      " -1  1  0 -1]\n",
      "test slant -0.472223315070823 [-0.41517013 -0.18800067 -1.92493456 -0.67323552 -0.80315049]\n",
      "[ 0 -1  0 -1  0  0 -1 -1 -1 -1 -1  0 -1 -1 -1 -1  1  1 -1 -1 -1 -1  1 -1\n",
      " -1 -1  0 -1 -1 -1 -1 -1 -1 -1  0  1  0 -1 -1  1  0 -1  0 -1 -1  0 -1 -1\n",
      " -1 -1 -1 -1  1 -1 -1  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1\n",
      " -1 -1 -1  0 -1  1  0  1  0 -1 -1 -1 -1  1  0  0 -1 -1 -1 -1  1  0 -1 -1\n",
      " -1 -1  1  0]\n",
      "test slant -0.6182775008626112 [-0.16252931  0.37794252 -3.96503446  0.82937558 -0.6654564 ]\n",
      "self_training_boosting_0.6_unbias\n",
      "[ 1  1  0 -1  0 -1  0  0 -1  0 -1 -1 -1  1 -1 -1 -1  1  1  0  1  1  1 -1\n",
      " -1 -1 -1  0  1  1 -1  1  0  0  0  0  1 -1  1 -1  1  0 -1  1 -1  1  1  0\n",
      "  0 -1  0  1  0  0  0  1  0 -1  0  0  0  0 -1 -1  1 -1  1 -1 -1  1  1 -1\n",
      " -1  1 -1 -1 -1 -1  1 -1  1 -1  1 -1 -1 -1 -1 -1  1 -1 -1  0 -1  0  1  0\n",
      " -1  1 -1  0]\n",
      "test slant -0.37843657172161604 [-0.59744773 -2.21534911 -0.1840196  -0.41194438 -0.7153484 ]\n",
      "[ 1  0 -1  0  0  1 -1 -1  1 -1 -1  0  0 -1  1  0  1 -1  0 -1 -1  0  1  1\n",
      "  1  0  0  0  0  1  0  0 -1 -1  0  1 -1  1  1 -1  1 -1  0  0  1  0  1  0\n",
      " -1 -1  0 -1  1 -1  1 -1 -1 -1  0 -1 -1  0  1  0  0  1 -1  0  1  0 -1  1\n",
      "  0 -1 -1 -1  1 -1 -1 -1 -1  1  1  1  0  0  1 -1 -1 -1  0  0 -1 -1 -1  0\n",
      " -1  1  0  1]\n",
      "test slant -0.37891107234162175 [ 1.16537465 -1.11384169 -0.09120035 -0.69285378 -0.49280287]\n",
      "[ 0 -1  1  1 -1 -1  0 -1 -1 -1  0  0  1 -1  1  1 -1  0  1  0  0  1 -1  0\n",
      " -1 -1  0  0 -1 -1  1 -1  1 -1  0  1  1 -1  0 -1  1  0 -1 -1 -1 -1  0  0\n",
      "  1  1  0  0 -1 -1  1 -1 -1  0  0  1  1 -1  0 -1  0 -1 -1 -1  0 -1  1  1\n",
      "  1  1 -1  0  1  0 -1  0 -1  0  0  0 -1 -1  1  0 -1 -1 -1 -1 -1  0  0 -1\n",
      " -1 -1 -1 -1]\n",
      "test slant -0.32153871445869836 [ 0.70484152 -1.89543218 -0.4658795  -1.59851492 -0.63625507]\n",
      "[-1  1 -1 -1  1  1 -1  1 -1  0  1 -1 -1  0  0 -1  0  1  0  1  0  1 -1  0\n",
      " -1  1  0  1 -1 -1  1 -1 -1 -1 -1 -1 -1  0  1  0 -1  1  1  1  1 -1  1  1\n",
      " -1 -1  1  1 -1  1  1  0 -1  0  1  1 -1 -1 -1 -1  0  1  0 -1  1  1  0 -1\n",
      "  0 -1  0  0 -1  0  1  1 -1 -1 -1  0 -1  0  0  1 -1  0  0  0 -1 -1 -1  1\n",
      "  0 -1  1  0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test slant -0.4208508003622189 [ 0.69316757  0.99336055 -0.56573177 -1.0006869  -0.31553989]\n",
      "[ 1 -1 -1  0  0 -1 -1  0  1  0  0  0 -1 -1 -1 -1  0 -1  0  1  1  1  1  0\n",
      " -1  0  0  1 -1  1 -1  1 -1  0 -1  1  0  1  0  0  1 -1  0  1  1  1  0 -1\n",
      "  0  0  0 -1  1  1  0  1  0  1  0  0  1  0  0  1 -1  0  0 -1 -1 -1  1  0\n",
      " -1  1  0 -1 -1 -1  1  0  1  1  0  1 -1  1  0  0  0 -1 -1 -1 -1  1  0  0\n",
      " -1  1  0 -1]\n",
      "test slant -0.3580147867714902 [-0.55148795  0.09343075 -1.01179395  0.51398214 -0.42096333]\n",
      "self_training_boosting_0.8_unbias\n",
      "[ 0  0  1  1  1  0 -1 -1  0  0  1  0  1  0  0  1  1  1  0 -1  0  0  0  1\n",
      "  1  0  0  1  0  0  1  1  0  0  1  1  1  1  0  1  0  0  1 -1  1  1  0  0\n",
      "  0  1 -1  1  0  0  1  0  0  0  0 -1  0  1  0  0  1 -1 -1  1  1  1  1  0\n",
      "  0  0  0 -1 -1  1  0 -1 -1 -1 -1  0  0  0  0  0  0  1  1  1  0  0  0  0\n",
      "  1  0  0 -1]\n",
      "test slant -0.264854084269058 [-0.18616553 -0.56605116  0.25763439 -0.34547197  0.64718679]\n",
      "[ 1  0  1  1  0  0  0 -1  0  1  0  0 -1  0 -1  1  1  1  0  0  1  0  1  1\n",
      "  0  0  1  0  0  1  0  0 -1  0  0 -1  0  0  0  1  1 -1  0  1  0  0  1  0\n",
      "  1  0 -1  1 -1  0  0 -1  0  0  0  0 -1  0  1  1 -1  0  0  0 -1 -1  0  0\n",
      "  1  1  0 -1  0  0  0  0  1  0  1 -1  0  0  0  1  0  1  0  0  0 -1 -1  0\n",
      "  0  1  0  0]\n",
      "test slant -0.2562676019916091 [ 0.63454419 -0.82307084 -1.95974047 -0.37421274  0.03735298]\n",
      "[ 0 -1  1  0 -1  0  1  0  1  0  1  1  0  0  0 -1  1 -1  1  1  0  0  0  0\n",
      "  0  0  0  0  0  1  0  0  0  0  0  0 -1  1  1 -1  0  0  0  0  0  1  1  0\n",
      "  0  0  0  1  0  0  0  1  0  1  1  1  1  1  0  0  1  0  1  0  1  0  0  0\n",
      " -1  1 -1 -1 -1  1  1  0 -1  1  0  1  0  0  0  0  1  0  0  1 -1  1  1 -1\n",
      "  1 -1  1  1]\n",
      "test slant -0.23476055301326187 [ 0.84770429 -0.3024962   0.27203647  0.62602316 -0.45260164]\n",
      "[ 1  1  1 -1 -1  0  1  0  1  0  1  1  1  0  1  1  1  1  1  0 -1 -1  0  0\n",
      "  0 -1  1  1 -1  0  0  1  0  0  1  0  1 -1  0 -1  1  0  0  0  0  0  0 -1\n",
      "  0  0  1  0 -1  0  1  0  0  0  0  1  1  1  1  1  0  1  1  1  1  1  0  1\n",
      "  0  0  1  1 -1 -1  0  1  0  0  1  1  1  0  0  1  0 -1  0  0  0  1 -1  0\n",
      " -1  1 -1  0]\n",
      "test slant -0.23906513355670408 [-0.45700025 -0.24042487 -0.21114863 -0.44260247  0.45044113]\n",
      "[-1  0  0  0  0  0  1 -1  1  0  1  1 -1  1 -1  1  0 -1  1  1  1 -1 -1 -1\n",
      "  0  0 -1  0  0 -1  0 -1  0  1 -1  1 -1  1  1 -1  1  1  0  1  0  0  0  0\n",
      "  0  1 -1  0  0  0  1  1  0  1  0  0  1  1  0  0  1  1 -1 -1  1  0  1  0\n",
      " -1  0  0  1  0 -1 -1 -1  0  0  1 -1  1  0  0  0 -1  0  0  1  0  1  0  1\n",
      "  1 -1  0 -1]\n",
      "test slant -0.22304621980887512 [-1.20086965  0.01891847 -0.15143597 -0.27874997 -0.30974677]\n"
     ]
    }
   ],
   "source": [
    "rounds = 5\n",
    "notmasks = [0.01, 0.03, 0.05, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# notmasks = [0.8]\n",
    "topn = 5000000\n",
    "\n",
    "\n",
    "model_name = 'self_training_boosting'\n",
    "\n",
    "\n",
    "# notmasks = [0.01]\n",
    "# topn = 5000\n",
    "# rounds = 2\n",
    "\n",
    "# svc = SVC(probability=True, gamma=\"auto\")\n",
    "clf = GBC(n_estimators=75, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "self_training_model = SelfTrainingClassifier(clf)\n",
    "# for biased in [True, False]:\n",
    "for biased in [True, False]:\n",
    "  bia = 'bias'\n",
    "  if not biased:\n",
    "    bia = 'unbias'\n",
    "  f = open( r'D:\\projects\\congress\\results\\debatepol\\train\\SSL/{}_{}.txt'.format(model_name, bia), 'w')\n",
    "  f.write('\\taccs\\tpccs\\n')\n",
    "  for notmask in notmasks:\n",
    "    version = \"{}_{}_{}\".format(model_name, notmask, bia)\n",
    "    print(version)\n",
    "    \n",
    "    if biased:\n",
    "      accs = []\n",
    "      pccs = []\n",
    "      for __ in range(rounds):\n",
    "        trainid, testid = sample(parties_all, theta_categ, stance_all, biased = biased, notmask = notmask, extreme = extreme)\n",
    "        ys = np.copy(parties_all)\n",
    "        ys[testid] = -1\n",
    "\n",
    "        Xs = np.copy(ds_all)\n",
    "        Xidx = np.arange(len(Xs))\n",
    "      \n",
    "        np.random.shuffle(Xidx)\n",
    "        Xs = Xs[Xidx]\n",
    "        ys = ys[Xidx]\n",
    "        print(ys[:100])\n",
    "        self_training_model.fit(Xs[:topn], ys[:topn])\n",
    "        Xt = ds_all[testid]\n",
    "        yt = parties_all[testid]\n",
    "        At = As_all[testid]\n",
    "        y_pred = self_training_model.predict(Xt)\n",
    "        proba_pred = self_training_model.predict_proba(Xt)\n",
    "        s_pred = np.array([np.log(i[1]*10001)- np.log(i[0]*1000) for i in proba_pred])\n",
    "\n",
    "        acc = np.mean(y_pred == yt)\n",
    "        accs.append(acc)\n",
    "        \n",
    "        pcc = predict_party_membership(s_pred, yt, At)\n",
    "        pccs.append(pcc)\n",
    "        \n",
    "        \n",
    "      f.write(version)\n",
    "\n",
    "      f.write('\\t' + str(accs))\n",
    "      f.write('\\t' + str(pccs))\n",
    "      print(\"acc, pcc\", accs, pccs)\n",
    "      f.write('\\n')\n",
    "    else:\n",
    "      accs = []\n",
    "      pccs = []\n",
    "      for __ in range(rounds):\n",
    "        trainid, testid = sample(parties_all, theta_categ, stance_all, biased = biased, notmask = notmask, extreme = extreme)\n",
    "        ys = np.copy(parties_all)\n",
    "        ys[testid] = -1\n",
    "\n",
    "        Xs = np.copy(ds_all)\n",
    "        Xidx = np.arange(len(Xs))\n",
    "      \n",
    "        np.random.shuffle(Xidx)\n",
    "        Xs = Xs[Xidx]\n",
    "        ys = ys[Xidx]\n",
    "        print(ys[:100])\n",
    "        self_training_model.fit(Xs[:topn], ys[:topn])\n",
    "        Xt = ds_all[testid]\n",
    "        yt = parties_all[testid]\n",
    "        At = As_all[testid]\n",
    "        y_pred = self_training_model.predict(Xt)\n",
    "        proba_pred = self_training_model.predict_proba(Xt)\n",
    "        s_pred = np.array([np.log(i[1]*1000)- np.log(i[0]*1000) for i in proba_pred])\n",
    "\n",
    "        acc = np.mean(y_pred == yt)\n",
    "        accs.append(acc)\n",
    "        \n",
    "        pcc = predict_party_membership(s_pred, yt, At)\n",
    "        pccs.append(pcc)\n",
    "        \n",
    "        \n",
    "      f.write(version)\n",
    "\n",
    "      f.write('\\t' + str(accs))\n",
    "      f.write('\\t' + str(pccs))\n",
    "      print(\"acc, pcc\", accs, pccs)\n",
    "      f.write('\\n')\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label spreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounds = 5\n",
    "# notmasks = [0.01, 0.03, 0.05, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# # notmasks = [0.8]\n",
    "# topn = 500000\n",
    "# model_name = 'label_spreading'\n",
    "# label_prop_model = LabelSpreading(kernel= 'knn', n_neighbors=10)\n",
    "# # for biased in [True, False]:\n",
    "# for biased in [True, False]:\n",
    "#   bia = 'bias'\n",
    "#   if not biased:\n",
    "#     bia = 'unbias'\n",
    "#   f = open( r'D:\\projects\\congress\\results\\debatepol\\train\\SSL/{}_{}.txt'.format(model_name, bia), 'w')\n",
    "#   f.write('\\taccs\\tpccs\\n')\n",
    "#   for notmask in notmasks:\n",
    "#     version = \"{}_{}_{}\".format(model_name, notmask, bia)\n",
    "#     print(version)\n",
    "\n",
    "#     if biased:\n",
    "#       accs = []\n",
    "#       pccs = []\n",
    "#       for __ in range(rounds):\n",
    "#         trainid, testid = sample(parties_all, theta_categ, stance_all, biased = biased, notmask = notmask, extreme = extreme)\n",
    "#         ys = np.copy(parties_all)\n",
    "#         ys[testid] = -1\n",
    "\n",
    "#         Xs = np.copy(ds_all)\n",
    "#         Xidx = np.arange(len(Xs))\n",
    "      \n",
    "#         np.random.shuffle(Xidx)\n",
    "#         Xs = Xs[Xidx]\n",
    "#         ys = ys[Xidx]\n",
    "#         print(ys[:100])\n",
    "#         label_prop_model.fit(Xs[:topn], ys[:topn])\n",
    "#         Xt = ds_all[testid]\n",
    "#         yt = parties_all[testid]\n",
    "#         At = As_all[testid]\n",
    "#         y_pred = label_prop_model.predict(Xt)\n",
    "#         proba_pred = label_prop_model.predict_proba(Xt)\n",
    "#         s_pred = np.array([np.log(i[1]*1000+ 0.00000001)- np.log(i[0]*1000+0.00000001) for i in proba_pred])\n",
    "\n",
    "#         acc = np.mean(y_pred == yt)\n",
    "#         accs.append(acc)\n",
    "        \n",
    "#         pcc = predict_party_membership(s_pred, yt, At)\n",
    "#         pccs.append(pcc)\n",
    "        \n",
    "        \n",
    "#       f.write(version)\n",
    "\n",
    "#       f.write('\\t' + str(accs))\n",
    "#       f.write('\\t' + str(pccs))\n",
    "#       f.write('\\n')\n",
    "#     else:\n",
    "#       accs = []\n",
    "#       for __ in range(rounds):\n",
    "#         trainid, testid = sample(parties_all, theta_categ, stance_all, biased = biased, notmask = notmask, extreme = extreme)\n",
    "#         ys = np.copy(parties_all)\n",
    "#         ys[testid] = -1\n",
    "\n",
    "#         Xs = np.copy(ds_all)\n",
    "#         Xidx = np.arange(len(Xs))\n",
    "      \n",
    "#         np.random.shuffle(Xidx)\n",
    "#         Xs = Xs[Xidx]\n",
    "#         ys = ys[Xidx]\n",
    "#         print(ys[:100])\n",
    "#         label_prop_model.fit(Xs[:topn], ys[:topn])\n",
    "#         Xt = ds_all[testid]\n",
    "#         yt = parties_all[testid]\n",
    "#         At = As_all[testid]\n",
    "#         y_pred = label_prop_model.predict(Xt)\n",
    "#         proba_pred = label_prop_model.predict_proba(Xt)\n",
    "#         s_pred = np.array([np.log(i[1]*1000+ 0.00000001)- np.log(i[0]*1000+0.00000001) for i in proba_pred])\n",
    "\n",
    "#         acc = np.mean(y_pred == yt)\n",
    "#         accs.append(acc)\n",
    "        \n",
    "#         pcc = predict_party_membership(s_pred, yt, At)\n",
    "#         pccs.append(pcc)\n",
    "        \n",
    "        \n",
    "#       f.write(version)\n",
    "\n",
    "#       f.write('\\t' + str(accs))\n",
    "#       f.write('\\t' + str(pccs))\n",
    "#       f.write('\\n')\n",
    "#   f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "labelspreading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
